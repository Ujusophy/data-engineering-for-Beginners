# Data Engineering for Beginners

> Learn data engineering from scratch through hands-on projects and clear explanations.

## About

This repository provides a structured path to learning data engineering, from basic concepts to advanced topics. Each module includes theory, exercises, and practical projects.

## What You'll Learn

- Python and SQL fundamentals
- Database design and data modeling
- Building ETL/ELT pipelines
- Working with Apache Airflow and Spark
- Stream processing with Kafka
- Cloud data platforms (AWS, GCP, Azure)
- Data quality and testing

## Getting Started

### Prerequisites
- Python 3.9 or higher
- Basic command line knowledge
- Git

### Quick Setup

```bash
git clone https://github.com/Ujusophy/data-engineering-for-Beginners.git
cd data-engineering-for-Beginners
```

Start with the [fundamentals module](01-fundamentals/) and work through each section in order.

## Curriculum

1. **Fundamentals** - Python, SQL, core concepts
2. **Databases** - PostgreSQL, data modeling
3. **Data Warehousing** - Dimensional modeling, ETL patterns
4. **Orchestration** - Apache Airflow
5. **Big Data** - Apache Spark
6. **Streaming** - Apache Kafka
7. **Cloud** - AWS
8. **Advanced** - Data quality, optimization
9. **Capstone Projects** - End-to-end implementations

## Repository Structure

```
├── 01-fundamentals/
├── 02-databases/
├── 03-data-warehousing/
├── 04-orchestration/
├── 05-big-data/
├── 06-streaming/
├── 07-cloud/
├── 08-advanced/
├── 09-capstone-projects/
├── datasets/
└── resources/
```

## Contributing

Contributions are welcome! 
